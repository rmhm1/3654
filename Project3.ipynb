{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmhm1/3654/blob/main/Project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di4kSRNb3D-D"
      },
      "source": [
        "# Title: It's an Avocado... Thanks!\n",
        "# Team Name: Free shAvacado\n",
        "##### Team Members: Shannon Aikens, Paolo Fermin, Ryland Hanson, Somya Jain \n",
        "##### PIDs: shannonaikens, rmhm1, somyajain17, paolofermin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCKwmJil3VvR"
      },
      "source": [
        "# Topic: Avocados and their Relation to the Economy\n",
        "#### Description:\n",
        "Millenials and Generation Z are often criticized for spending their income on unnecessary items. This can include expensive food items such as avocados. We want to know if the sale price and volume at which avocados are purchased reflects the state of the economy.  \n",
        "\n",
        "\n",
        "#### Potential Research Questions:\n",
        "1.   How does avocado purchasing relate to the housing market in different regions?\n",
        "2.   How does avocado purchasing relate to average income in different regions?\n",
        "3.   How do internet trends related to avocados affect the avocado market?\n",
        "\n",
        "\n",
        "#### Motivation\n",
        "We want to see if spending more money on avocados has a positive correlation with income. We also want to evaluate the claims that spending more money on avocados takes away from important investments like buying a house. We want to compare this data to the overall sentiment of avocados on social media platforms to determine if the claims about avocado spending relate to avocado purchasing patterns. This research can be applicable to other products that are perceived to be more or less popular among younger generations.\n",
        "\n",
        "\n",
        "\n",
        "#### Relevant Data (Methods):\n",
        "https://www.kaggle.com/timmate/avocado-prices-2020\n",
        "This data set contains the prices, volumes, dates, and regions related to avocados. \n",
        "\n",
        "https://www.kaggle.com/alexphoffman/big-city-land-values-and-walkscores\n",
        "This data set contains the walk score and land values for several big cities in the United States. \n",
        "\n",
        "https://www.census.gov/construction/nrs/historical_data/index.html\n",
        "US Census datasets on housing sales by price range, region, and type of financing.\n",
        "\n",
        "https://fred.stlouisfed.org/release/tables?rid=249&eid=259515#snid=259516\n",
        "This webpage was used to create a data set contains the median income for each state along with D.C from the year 2019.\n",
        "\n",
        "https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2018-01-01#\n",
        "This webpage was used to create a data set contains the median income for each state along with D.C from the year 2018.\n",
        "\n",
        "https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2017-01-01#\n",
        "This webpage was used to create a data set contains the median income for each state along with D.C from the year 2017.\n",
        "\n",
        "https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2016-01-01#\n",
        "This webpage was used to create a data set contains the median income for each state along with D.C from the year 2016.\n",
        "\n",
        "https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2015-01-01#\n",
        "This webpage was used to create a data set contains the median income for each state along with D.C from the year 2015.\n",
        "\n",
        "\n",
        "\n",
        "Other relevant data would include tweets where hastags related to avocados are used which we would scrape from twitter. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc0DdQXy6_p7"
      },
      "source": [
        "## Data Acquisition and Processing\n",
        "\n",
        "### Between our four members, we found datasets on different economic indicators and avocado trends with the hopes of finding connections between them later on in the project. Paolo worked with an avocado dataset and found quantitative trends while Ryland found more qualitative trends through Twitter web scraping. Somya was able to web scrape income data from years corresponding to the avocado dataset and Shannon was able to combine multiple datasets on housing sales by price and region.\n",
        "\n",
        "### The acquisition and processing of this data covers all three requirements. Somya and Ryland met requirement 1 with web scraping, Paolo and Shannon met requirement 2 by having datasets large enough to sufficiently answer questions, and Shannon and Somya met requirement 3 by combining multiple datasets to get a more unique perspective of their topic.\n",
        "\n",
        "### With this collected data, we hope to find correlations between economic trends and avocado trends in the next phase through things like sentiment analysis, clustering, and regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6udW9tU73F4"
      },
      "source": [
        "### Dataset 1: Avocado Prices (Paolo Fermin)\n",
        "\n",
        "#### This data was found already in csv form through the popular dataset website Kaggle. It covers the years 2015-2020 and contains data on avocado purchasing volume and pricing within the United States. Some attributes are not likely to be helpful in the analysis (such as the specific id number of the avocado, or the size of the bag that they were puchased as), but I elected not to remove them just in case they were relevant. \n",
        "\n",
        "#### The \"cleaning\" that was necessary for this dataset did not have to do with messy data. There were no NaN values or strings where there should be numerical values. Instead, the data that is too specific. The \"Geography\" column contains very fine-grained detail of precisely what location the avocado purchases were made. I created a dictionary that maps specific location to a set of regions, corresponding to the U.S. Census regions that the Housing Data was given in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiqs6bcW7taO"
      },
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr2RLXufA3hn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "3cbcea41-15ea-4881-fa85-38d1f46ca42d"
      },
      "source": [
        "avocados = pd.read_csv('avocado-updated-2020.csv')\n",
        "avocados.dtypes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-01bab30ce085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavocados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avocado-updated-2020.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mavocados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'avocado-updated-2020.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sio5yrS0A5MA"
      },
      "source": [
        "# sort geography into regions\n",
        "# regions are based on U.S. census distributions\n",
        "\n",
        "region_mapping = {\n",
        "    'Northeast': ['Albany', 'Boston', 'Buffalo/Rochester', 'Harrisburg/Scranton', \n",
        "                      'Hartford/Springfield', 'New York', 'Northeast', 'Northern New England', 'Philadelphia', 'Pittsburgh', \n",
        "                     'Syracuse', ],\n",
        "    \n",
        "    'Midwest': ['Chicago', 'Cincinnati/Dayton', 'Columbus', 'Detroit', 'Grand Rapids', 'Great Lakes', 'Indianapolis', 'Plains',\n",
        "                   'St. Louis'],\n",
        "    \n",
        "    'South': ['Atlanta', 'Baltimore/Washington', 'Charlotte', 'Dallas/Ft. Worth', 'Houston', 'Jacksonville', 'Louisville', \n",
        "              'Miami/Ft. Lauderdale', 'Midsouth', 'Nashville', 'New Orleans/Mobile', 'Orlando', 'Raleigh/Greensboro', 'Richmond/Norfolk',\n",
        "                 'Roanoke', 'South Carolina', 'South Central', 'Southeast', 'Tampa', ],\n",
        "    \n",
        "    'West': ['Boise', 'California', 'Denver', 'Las Vegas', 'Los Angeles', 'Phoenix/Tucson', 'Portland', 'Sacramento', 'San Diego', \n",
        "                'San Francisco', 'Seattle', 'Spokane', 'West', 'West Tex/New Mexico'],\n",
        "    \n",
        "    'US': ['Total U.S.']\n",
        "}\n",
        "\n",
        "# \"flatten\" the dictionary so that each location has it's own separate key:value pair (corresponding to location:region)\n",
        "flat_regions = {val:key for key, lst in region_mapping.items() for val in lst}\n",
        "flat_regions\n",
        "\n",
        "avocados['Region'] = avocados['geography'].map(flat_regions)\n",
        "avocados.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvP7mZVFA7Tb"
      },
      "source": [
        "# write the avocado data to csv\n",
        "avocados.to_csv(os.path.join(os.getcwd(), 'clean_avocados.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKk5Scz77XRH"
      },
      "source": [
        "### Dataset 2: Income Data (Somya Jain)\n",
        "\n",
        "#### I went on the FRED website and scraped information on median income for states for the years 2015 to 2019 since those are the years covered by the Avocado dataset. The incomes and state names were stored in a table so I searched for 'td' tags and cleaned the values before putting them into a list. I used those lists to create a dataframe which was then written to a csv file.\n",
        "\n",
        "#### This uses the first and third data requirements since I scraped information from multiple pages and put them in the same csv.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uxIMRC57t93"
      },
      "source": [
        "## importing libraries\n",
        "import requests\n",
        "import bs4\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeTy7gX6LN6p"
      },
      "source": [
        "def get_incomes(soup):\n",
        "    '''\n",
        "    Scrape FRED webpage for data on income in a given year.\n",
        "    \n",
        "    params:\n",
        "        soup - soup object that needs to be parsed through\n",
        "    return:\n",
        "        list_income - list of the incomes found on the page\n",
        "    '''\n",
        "    \n",
        "    list_income = []\n",
        "    s = soup.find_all('td', class_ = 'fred-rls-elm-vl-td')\n",
        "\n",
        "    ## creating a list of mediam income in each state from 2019\n",
        "    temp = []\n",
        "    n = 3\n",
        "    \n",
        "    ## gets only income for year on page\n",
        "    for i, item in enumerate(s):\n",
        "        if i%n == 0:\n",
        "            temp.append(item.text)\n",
        "\n",
        "    ## cleans value and appends it to list as an int\n",
        "    for item in temp[1:]:\n",
        "        num = []\n",
        "        for char in item:\n",
        "            if char.isdigit():\n",
        "                num.append(char)\n",
        "        income = ''.join(num)\n",
        "        list_income.append(int(income))\n",
        "    \n",
        "    return list_income"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82-4vmcxLRjK"
      },
      "source": [
        "## getting page information for 2019 incomes\n",
        "url_2019 = '''https://fred.stlouisfed.org/release/tables?rid=249&eid=259515#snid=259516'''\n",
        "soup_2019 = bs4.BeautifulSoup(requests.get(url_2019).text, 'html5lib')\n",
        "list_income_2019 = get_incomes(soup_2019)\n",
        "\n",
        "## getting page information for 2018 incomes\n",
        "url_2018 = '''https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2018-01-01#'''\n",
        "soup_2018 = bs4.BeautifulSoup(requests.get(url_2018).text, 'html5lib')\n",
        "list_income_2018 = get_incomes(soup_2018)\n",
        "\n",
        "## getting page information for 2017 incomes\n",
        "url_2017 = '''https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2017-01-01#'''\n",
        "soup_2017 = bs4.BeautifulSoup(requests.get(url_2017).text, 'html5lib')\n",
        "list_income_2017 = get_incomes(soup_2017)\n",
        "\n",
        "## getting page information for 2016 incomes\n",
        "url_2016 = '''https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2016-01-01#'''\n",
        "soup_2016 = bs4.BeautifulSoup(requests.get(url_2016).text, 'html5lib')\n",
        "list_income_2016 = get_incomes(soup_2016)\n",
        "\n",
        "## getting page information for 2015 incomes\n",
        "url_2015 = '''https://fred.stlouisfed.org/release/tables?rid=249&eid=259515&od=2015-01-01#'''\n",
        "soup_2015 = bs4.BeautifulSoup(requests.get(url_2015).text, 'html5lib')\n",
        "list_income_2015 = get_incomes(soup_2015)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLyrjkPELVI_"
      },
      "source": [
        "## gets the list of states that income is being found for\n",
        "list_states = []\n",
        "states_soup = soup_2019.find_all('span', class_ = 'fred-rls-elm-nm')\n",
        "[list_states.append(state.text) for state in states_soup[1:]]\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNaJPd0ILgGr"
      },
      "source": [
        "## puts data into data frame\n",
        "df = pd.DataFrame({\n",
        "    \n",
        "    'State': list_states,\n",
        "    '2019 Median Income': list_income_2019,\n",
        "    '2018 Median Income': list_income_2018,\n",
        "    '2017 Median Income': list_income_2017,\n",
        "    '2016 Median Income': list_income_2016,\n",
        "    '2015 Median Income': list_income_2015\n",
        "})\n",
        "\n",
        "## reads data to a csv file\n",
        "#df.to_csv('IncomeDate2015to2019.csv', index=False)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcKHA6mL7hts"
      },
      "source": [
        "### Dataset 3: Housing Market Data (Shannon Aikens)\n",
        "\n",
        "#### I was able to use U.S. Census data to find housing sales by region and price. These datasets will work together with the Avocado dataset to determine spending trends for both avocados and houses based on region. The housing sales by region will also work together with the scraped internet data on avocado opinions by region. \n",
        "\n",
        "#### I was able to export the U.S. Census tables as csv files and format them as dataframes. The U.S. Census gave me data going all the way back to the 1960s, but I narrowed the scope of the datasets to the 2000s to match the Avocado, Income, and Internet datasets. I was able to change all data to be quantitative to prepare for future analysis techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSYIfgJg7ueP"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "regionDF = pd.read_csv('pricereg_cust.csv')\n",
        "priceDF = pd.read_csv('qtrsalgrp_us_ann.csv')\n",
        "\n",
        "display(regionDF.head(10))\n",
        "display(priceDF.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANhAt6zjjl7B"
      },
      "source": [
        "#getting relevant rows\n",
        "regionDF = regionDF.iloc[4:,:]\n",
        "\n",
        "#creating column names\n",
        "columns = ['Year','UnitedStatesMedian','NortheastMedian','MidwestMedian','SouthMedian','WestMedian',\n",
        "          'UnitedStatesAverage', 'NortheastAverage', 'MidwestAverage','SouthAverage','WestAverage']\n",
        "regionDF.columns = columns\n",
        "\n",
        "#resetting index\n",
        "regionDF = regionDF.reset_index(drop=True)\n",
        "\n",
        "#cleaning data values\n",
        "\n",
        "#looping through quantitative columns\n",
        "for column in columns:\n",
        "    \n",
        "    #replacing commas in each column with empty string\n",
        "    for i in range(len(regionDF)):\n",
        "        regionDF.loc[i,column] = regionDF.loc[i,column].replace(',','')\n",
        "        \n",
        "    #changing type of column to integer\n",
        "    regionDF = regionDF.astype({column:'int64'})\n",
        "        \n",
        "\n",
        "#narrowing years to 2002-2020 to match price dataset\n",
        "regionDF = regionDF.iloc[39:]\n",
        "regionDF = regionDF.reset_index(drop=True)\n",
        "                            \n",
        "display(regionDF)\n",
        "\n",
        "#exporting cleaned dataframe to csv\n",
        "#regionDF.to_csv(r'C:\\Users\\shana\\OneDrive\\Documents\\Sophomore Year-Semester 2\\Data Visualization\\housing_sales_region.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZb9uVQjpiG"
      },
      "source": [
        "#getting relevant rows\n",
        "priceDF = priceDF.iloc[7:,:]\n",
        "\n",
        "#creating column names\n",
        "columns = ['Year','Total','Under 125,000','125,000 to 149,999','150,000 to 199,999','200,000 to 249,999',\n",
        "          '250,000 to 299,999', '300,000 to 399,999', '400,000 to 499,999','500,000 to 749,999','Over 750,000']\n",
        "priceDF.columns = columns\n",
        "\n",
        "#resetting index\n",
        "priceDF = priceDF.reset_index(drop=True)\n",
        "\n",
        "#cleaning data values\n",
        "\n",
        "#looping through quantitative columns\n",
        "for column in columns:\n",
        "    \n",
        "    #replacing commas in each column with empty string\n",
        "    for i in range(len(priceDF)):\n",
        "        priceDF.loc[i,column] = priceDF.loc[i,column].replace(',','')\n",
        "        \n",
        "    #changing type of column to integer\n",
        "    priceDF = priceDF.astype({column:'int64'})\n",
        "                            \n",
        "display(priceDF)\n",
        "\n",
        "#exporting cleaned dataframe to csv\n",
        "#priceDF.to_csv(r'C:\\Users\\shana\\OneDrive\\Documents\\Sophomore Year-Semester 2\\Data Visualization\\housing_sales_price')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRDPGayW7nnu"
      },
      "source": [
        "### Dataset 4: Web Scraping Twitter Mentions of Avocados (Ryland Hanson)\n",
        "\n",
        "#### In order to get around the twitter developer account requirement along with limits set by the API, I decided to use a library, twint, instead. Tweets mentioning avocados are scraped from various locations that we have economic data on over a large range of time to see how the sentiment of avocados in these areas change over time and relate to the ecnonomy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsLv6US40yHr"
      },
      "source": [
        "#### Twint and nest_asyncio must be installed. Documentation with download instructions are below:\n",
        "https://github.com/twintproject/twint  \n",
        "https://pypi.org/project/nest-asyncio/\n",
        "\n",
        "Running the install commands below then restarting the Runtime should allow the twint module to be downloaded and working. The install commands need only be run the first time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "392FZ37E23qm"
      },
      "source": [
        "!pip install --user --upgrade -e git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
        "#!pip3 install nest_asyncio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULNdNK2huqgS"
      },
      "source": [
        "import pandas as pd\n",
        "import twint\n",
        "import nest_asyncio\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# Un-comment below line to download the full stopword list. \n",
        "#nltk.download('stopwords') \n",
        "nest_asyncio.apply()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsf-qxEg2EUS"
      },
      "source": [
        "import string\n",
        "string.punctuation, string.digits\n",
        "\n",
        "## Function found on StackOverflow to allow the removal of emoji's through the use of regex\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "## Cleans tweets of emojis, punctuation, digits, then splits up into the bags of words and adds column to df:\n",
        "def clean_tweets(dataframe):\n",
        "    cleaned = dataframe['tweet'].str.replace('http\\S+|www.\\S+', '', case=False) # Removes URLS\n",
        "    cleaned = cleaned.map(lambda tweet: remove_emoji(tweet)) # Removes emojis\n",
        "    cleaned = cleaned.map(lambda txt: txt.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))) #Removes punc.\n",
        "    cleaned = cleaned.map(lambda txt: txt.translate(str.maketrans(string.digits, ' '*len(string.digits)))) # Removes digits\n",
        "    bag = cleaned.map(lambda txt: txt.lower().strip().split()) # Splits the words into the different bags\n",
        "    bag = bag.map(lambda word_tokens: [w for w in word_tokens if not w in stop_words]) # Remove stop words\n",
        "\n",
        "    dataframe['bag'] = bag\n",
        "    return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpKXSncT2XOq"
      },
      "source": [
        "# Creates df to hold all tweets over the different regions. \n",
        "tweets_df = pd.DataFrame(columns = ['date', 'place', 'username', 'tweet', 'nlikes', 'city'])\n",
        "\n",
        "# Loop over a list of cities to scrape tweets with geo stamps of the target city\n",
        "for city in ['new york', 'los angeles', 'chicago', 'houston', 'miami', 'atlanta', 'charlotte']:\n",
        "    # Configure the tweet searching. Attributes to be tweaked.\n",
        "    c = twint.Config()\n",
        "    c.Search = ['avocado']\n",
        "    #c.Popular_tweets = False\n",
        "    #c.Min_likes = 1\n",
        "    c.Pandas=True\n",
        "    c.Lang = 'en'\n",
        "    c.Near = city\n",
        "    c.Since = '2015-01-01'\n",
        "    c.Until = '2019-12-30'\n",
        "    c.Hide_output = True\n",
        "    # Runs the search using the configuration above\n",
        "    twint.run.Search(c)\n",
        "    # Grabs the df made from twint, adds a target city column, then appends it to the overall df.\n",
        "    temp_df = twint.storage.panda.Tweets_df[['date', 'place', 'username', 'tweet', 'nlikes']]\n",
        "    temp_df['city'] = city\n",
        "    tweets_df = tweets_df.append(temp_df, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmRUvmkw2phS"
      },
      "source": [
        "# Clean the tweets, remove the stop words, then set equal to cleaned_tweets\n",
        "cleaned_tweets = clean_tweets(tweets_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Srf6aGrBMkl"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAjJTJ54Gr8Z"
      },
      "source": [
        "### Inelastic vs. Elastic Goods (Paolo Fermin)\n",
        "\n",
        "#### One of the biggest motivations for this question is whether or not young people are spending too much on avocados, which are often seen as an expensive good. To test this theory, we can look at the \"price elasticity of demand\", an economic concept that essentially asks how the demand changes based upon the price. \n",
        "\n",
        "#### The demand for an \"elastic\" good changes as the price changes. For example, fewer people will buy the same good when it becomes more expensive. Intuitively, most goods are elastic. However, some goods such as water and gasoline are \"inelastic\" - people will continue to buy those goods, no matter what the price is. The common narrative of the media suggests that young people see avocados as inelastic goods, and will continue to buy them at exorbitant prices because they don't know any better. \n",
        "\n",
        "#### One can determine the elasticity of a good by plotting the price vs. volume of a good. If there is a highly negative correlation between the two, then the good is elastic - as price rises, volume falls. Conversely, if there is a low correlation between the two, then the good can be considered inelastic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3TEoFLsG8fP"
      },
      "source": [
        "# one of the first things to note is the split between conventional and organic avocados\n",
        "# simply plotting the mean price of conventional avocados vs organic avocados should demonstrate this\n",
        "conventional = avocados[(avocados.type == 'conventional')]\n",
        "us_conv = conventional.loc[conventional.geography == 'Total U.S.', :]\n",
        "organic = avocados[(avocados.type == 'organic')]\n",
        "us_org = organic.loc[organic.geography == 'Total U.S.', :]\n",
        "\n",
        "bar_df = pd.DataFrame({'Avocado Type': ['Conventional', 'Organic'], 'Average Price': [conventional.average_price.mean(), organic.average_price.mean()]})\n",
        "ax = bar_df.plot.bar(x='Avocado Type', y='Average Price')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(f'{p.get_height():0.02f}'), (p.get_x() * 1.05, p.get_height() * 1.005), ha='center')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWqtCDt_4-tW"
      },
      "source": [
        "#### This simple bar graph shows that there are vast differences in price between different types of avocados. In order to perform a reasonable economic analysis, only one of the two types should be picked. \n",
        "\n",
        "#### Let us look at organic avocados specifically, since they can be considered more of a luxury good. If organic avocados are inelastic, then conventional avocados would most certainly be inelastic as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9nLj24f470a"
      },
      "source": [
        "# group organic data by geography and sort by highest correlation between average_price and total_volume\n",
        "org_geo_groups = organic.groupby('geography')\n",
        "corrs = []\n",
        "for geo, df in org_geo_groups:\n",
        "    corrs.append(df.average_price.corr(df.total_volume))\n",
        "corr_df = pd.DataFrame({'geography':org_geo_groups.groups.keys(), 'correlation':corrs})\n",
        "corr_df.sort_values('correlation', inplace=True)\n",
        "corr_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "petNn0tV5Dec"
      },
      "source": [
        "fig, axs = plt.subplots(11, 5, figsize=(15, 30), facecolor='w', edgecolor='k')\n",
        "fig.subplots_adjust(hspace=.5, wspace=.1)\n",
        "\n",
        "axs = axs.ravel()\n",
        "\n",
        "for i, (geo, corr) in enumerate(zip(corr_df.geography, corr_df.correlation)):\n",
        "    # plot a scatter plot price vs. volume for each location\n",
        "    df = org_geo_groups.get_group(geo)\n",
        "    df.plot.scatter(x='average_price', y='total_volume', ax=axs[i])\n",
        "    axs[i].ticklabel_format(axis=\"both\", style=\"sci\", scilimits=(0,0))\n",
        "    axs[i].set_title(geo)\n",
        "    \n",
        "    # plot the regression line \n",
        "    fit = np.polyfit(df.average_price, df.total_volume, 1)\n",
        "    f = np.poly1d(fit)\n",
        "    df.insert(len(df.columns), 'Regression', f(df.average_price))\n",
        "    df.plot(x='average_price', y='Regression', ax=axs[i], color='Red', legend=False)\n",
        "    \n",
        "    # list the actual regression\n",
        "    axs[i].text(0.9, 0.8, f'R2:{corr:.2f}', verticalalignment='bottom', horizontalalignment='right', \\\n",
        "                transform=axs[i].transAxes, color='Red')\n",
        "    \n",
        "    # put a ylabel only on the endpoints\n",
        "    if i % 5 != 0:\n",
        "        ylabel = axs[i].axes.get_yaxis().get_label()\n",
        "        ylabel.set_visible(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrhxAjAg5F7E"
      },
      "source": [
        "### Observations\n",
        "\n",
        "The demand for organic avocados is correlated to the price in some geographic locations in the country, but not others. \n",
        "\n",
        "Could this relate to the median income in these areas? Or perhaps sentiment analysis can find a correlation? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG-Oj-C1G2ae"
      },
      "source": [
        "### Income vs. Price Analysis of Avocados (Somya Jain)\n",
        "\n",
        "#### During my initial exploration of the data I found that avocado prices in all regions seem to follow similar trends. I wanted to figure out if changes in income affected those trends. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCiRX_L-G9Fi"
      },
      "source": [
        "# Reading in clean income data\n",
        "incomes = pd.read_csv('IncomeDate2015to2019.csv')\n",
        "\n",
        "# Reading in clean avocado data\n",
        "avocados = pd.read_csv('cleaned_avocados.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ764GFGaizv"
      },
      "source": [
        "Need to add region to income data so it corresponds to the correct avocado data\n",
        "Regions are based on U.S. census distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnmkCEz7agBN"
      },
      "source": [
        "# adding regions to income data\n",
        "\n",
        "# dictionary of states in each region\n",
        "region_mapping = {\n",
        "    'Northeast': ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont', 'New Jersey', 'New York', 'Pennsylvania' ],\n",
        "    \n",
        "    'Midwest': ['Illinois', 'Indiana', 'Michigan', 'Ohio', 'Wisconsin', 'Iowa', 'Kansas', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'South Dakota'],\n",
        "    \n",
        "    'South': ['Delaware', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia', 'District of Columbia', 'West Virginia','Alabama', 'Kentucky', 'Mississippi', 'Tennessee', 'Arkansas', 'Louisiana', 'Oklahoma', 'Texas'],\n",
        "    \n",
        "    'West': ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming', 'Alaska', 'California', 'Hawaii', 'Oregon', 'Washington'],\n",
        "\n",
        "    'US': ['US']\n",
        "}\n",
        "\n",
        "# adding Region column to incomes dataset\n",
        "incomes['Region'] = [0 for i in range(len(incomes))]\n",
        "index = 0\n",
        "\n",
        "# adding Region to rows in incomes dataset depeding on state\n",
        "for state in incomes['State']:\n",
        "    for region in region_mapping:\n",
        "        if state in region_mapping[region]:\n",
        "            incomes['Region'][index] = region\n",
        "            index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e9jmaftato5"
      },
      "source": [
        "# adding Year Column to avocados dataset\n",
        "years = [int(avocados['date'][index][-4:]) for index in range(len(avocados['date']))]\n",
        "avocados['Year'] = years\n",
        "\n",
        "# grouping avocados data by year and region\n",
        "avocados_grouped = avocados.groupby(['Year', 'Region']).mean()\n",
        "\n",
        "# formatting data into lists for ease of graphing\n",
        "y_midwest = [avocados_grouped['average_price'][i] for i in range(len(avocados_grouped)) if i%5 == 0 ][:-1]\n",
        "y_northeast = [avocados_grouped['average_price'][i] for i in range(len(avocados_grouped)) if (i-1)%5 == 0 ][:-1]\n",
        "y_south = [avocados_grouped['average_price'][i] for i in range(len(avocados_grouped)) if (i-2)%5 == 0 ][:-1]\n",
        "y_west = [avocados_grouped['average_price'][i] for i in range(len(avocados_grouped)) if (i-4)%5 == 0 ][:-1]\n",
        "\n",
        "xs = [2015, 2016, 2017, 2018, 2019]\n",
        "\n",
        "# plotting avocados prices by year\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(xs, y_midwest, color = 'red', label = 'Midwest')\n",
        "plt.plot(xs, y_northeast, color = 'green', label = 'Northeast')\n",
        "plt.plot(xs, y_south, color = 'blue', label = 'South')\n",
        "plt.plot(xs, y_west, color = 'pink', label = 'West')\n",
        "\n",
        "plt.legend(loc = 'upper left')\n",
        "\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('Average Avocado Prices')\n",
        "plt.title('Average Avocado Prices by Year')\n",
        "fig_prices = plt.gcf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu9bcKr3a1Qr"
      },
      "source": [
        "Avocado Prices seem to have peeked for all regions around 2017.\n",
        "\n",
        "Observations:\n",
        "Avocado prices in the Northeast were higher than any other region until around 2018.\n",
        "Around 2018, the West had higher avocado prices\n",
        "The prices seem to follow similar trends in all regions\n",
        "\n",
        "Can the incomes in those regions explain some of these observations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkypbZkba4v8"
      },
      "source": [
        "# grouping incomes by region\n",
        "incomes_grouped = incomes.groupby('Region').mean()\n",
        "\n",
        "# formatting data into lists for ease of graphing\n",
        "x_midwest = [incomes_grouped.iloc[0][i] for i in range(len(incomes_grouped.iloc[0]))]\n",
        "x_midwest.reverse()\n",
        "\n",
        "x_northeast = [incomes_grouped.iloc[1][i] for i in range(len(incomes_grouped.iloc[1]))]\n",
        "x_northeast.reverse()\n",
        "\n",
        "x_south = [incomes_grouped.iloc[2][i] for i in range(len(incomes_grouped.iloc[2]))]\n",
        "x_south.reverse()\n",
        "\n",
        "x_west = [incomes_grouped.iloc[3][i] for i in range(len(incomes_grouped.iloc[2]))]\n",
        "x_west.reverse()\n",
        "\n",
        "# plotting avocado prices vs income\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(x_midwest, y_midwest, color = 'red', label = 'Midwest')\n",
        "plt.scatter(x_northeast, y_northeast, color = 'green', label = 'Northeast')\n",
        "plt.scatter(x_south, y_south, color = 'blue', label = 'South')\n",
        "plt.scatter(x_west, y_west, color = 'pink', label = 'West')\n",
        "\n",
        "plt.legend(loc = 'upper left')\n",
        "\n",
        "plt.xlabel('Average Median Income')\n",
        "plt.ylabel('Average Avocado Prices')\n",
        "plt.title('Average Avocado Prices vs. Average Median Income')\n",
        "fig_incomes = plt.gcf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4ObwvaXa_LK"
      },
      "source": [
        "Observations:\n",
        "It seems as average median income increases the price of avocados increases. This does not appear to be a strong relationship. It also seems that the northeast generally has a higher average income which could account for prices being higher as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLnLNczVa8Ur"
      },
      "source": [
        "# combining all income lists\n",
        "x = []\n",
        "[x.append(i) for i in x_midwest]\n",
        "[x.append(i) for i in x_northeast]\n",
        "[x.append(i) for i in x_south]\n",
        "[x.append(i) for i in x_west]\n",
        "\n",
        "# combining all avocado price lists\n",
        "y = []\n",
        "[y.append(i) for i in y_midwest]\n",
        "[y.append(i) for i in y_northeast]\n",
        "[y.append(i) for i in y_south]\n",
        "[y.append(i) for i in y_west]\n",
        "\n",
        "incomes_prices = pd.DataFrame(\n",
        "    {\n",
        "        'incomes': x,\n",
        "        'prices': y\n",
        "    }\n",
        ")\n",
        "\n",
        "# getting correlation coefficient for prices and incomes\n",
        "corr = incomes_prices.incomes.corr(incomes_prices.prices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtHOR3aEbE_a"
      },
      "source": [
        "The overall correlation between the average median incomes and the average prices is about 0.53. This indicates that there is a positive correlation, but it is not strong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWilradHG-3Y"
      },
      "source": [
        "### Geographic Analysis of Housing Market/Income And Avocado Prices (Shannon Aikens)\n",
        "\n",
        "#### Since the avocado, income, and housing market datasets all include regional data, I wanted to look at how similar the regions were based on the data we had collected. To do this, I used MDS with Manhattan Distances and plotted the regions to see how similar they were on a two-dimensional plane. I used MDS on a dataframe with avocado and housing market data, avocado and income data, and then all three datasets combined. The goal was to see if there was a trend among all three plots created or if any one factor (housing market or income) drastically changed the similarities between regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8tQwsRsHJIZ"
      },
      "source": [
        "### MDS on Avocado and Housing Market\n",
        "\n",
        "#grouping avocados dataset by region\n",
        "avocados_grouped = avocados.groupby('Region')\n",
        "\n",
        "#getting the mean price and volume for avocados\n",
        "price_averages_region = avocados_grouped.mean()['average_price'].reset_index()\n",
        "volume_averages_region = avocados_grouped.mean()['total_volume'].reset_index()\n",
        "\n",
        "#getting the median price and volume for avocados\n",
        "price_median_region = avocados_grouped.median()['average_price'].reset_index()\n",
        "volume_median_region = avocados_grouped.median()['total_volume'].reset_index()\n",
        "\n",
        "#creating regions dataframe that will be used to conduct MDS\n",
        "regionsDF = pd.DataFrame.from_dict({'Region': price_averages_region['Region'].tolist(), \n",
        "            'AvocadoMeanPrice': price_averages_region['average_price'],\n",
        "            'AvocadoMeanVolume': volume_averages_region['total_volume'], \n",
        "            'AvocadoMedianPrice': price_median_region['average_price'],\n",
        "            'AvocadoMedianVolume': volume_median_region['total_volume']})\n",
        "\n",
        "\n",
        "#############################################################################################################\n",
        "\n",
        "#getting quantitative data from housing price by region dataset\n",
        "regionDF_averages = regionDF.iloc[13:19,1:].mean().reset_index()\n",
        "regionDF_averages.columns = ['Region','CentralValue']\n",
        "\n",
        "#getting all mean and median values for the regions\n",
        "regionDF_medians = regionDF_averages.iloc[:5,:].sort_values('Region')\n",
        "regionDF_means = regionDF_averages.iloc[5:10,:].sort_values('Region')\n",
        "\n",
        "#converting mean and median values to lists to use in dataframe\n",
        "region_medians = regionDF_medians['CentralValue'].tolist()\n",
        "region_means = regionDF_means['CentralValue'].tolist()\n",
        "\n",
        "#adding housing mean and median prices for each region to dataframe\n",
        "regionsDF['HousingMeanPrice'] = region_means\n",
        "regionsDF['HousingMedianPrice'] = region_medians\n",
        "\n",
        "#reformatting the dataframe so it is indexed by region\n",
        "regionsDF.index = regionsDF['Region']\n",
        "regionsDF = regionsDF.iloc[:,1:]\n",
        "\n",
        "display(regionsDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMgM2jTHTw-t"
      },
      "source": [
        "import sklearn.metrics.pairwise as pairs\n",
        "import sklearn.manifold as mani\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Normalization\n",
        "norm = (regionsDF - regionsDF.mean())/regionsDF.std()\n",
        "\n",
        "#L1 Distance Matrix\n",
        "normHD = pairs.manhattan_distances(norm)\n",
        "norm = pd.DataFrame(normHD, columns=norm.index, index = norm.index)\n",
        "norm #high dimensional distances for every region\n",
        "\n",
        "#MDS calculations\n",
        "mds = mani.MDS(dissimilarity='precomputed', n_init=10, max_iter=1000)\n",
        "data2D = mds.fit_transform(norm)\n",
        "data2D = pd.DataFrame(data2D, columns=['x','y'], index=norm.index)\n",
        "data2D #suggested coordinates for plotting in two dimensions\n",
        "\n",
        "#Plotting coordinate suggestions\n",
        "ax = data2D.plot.scatter(x='x',y='y')\n",
        "for i in range(len(data2D)):\n",
        "    ax.text(data2D.x[i]+.2, data2D.y[i]-.1, data2D.index[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZSzbwF_T1MQ"
      },
      "source": [
        "### MDS on Avocado and Income Data\n",
        "\n",
        "#copying original income dataframe\n",
        "incomeDF = df.copy()\n",
        "\n",
        "#mapping states to regions\n",
        "regions_dict = {'Midwest': ['Illinois','Indiana','Iowa','Kansas','Michigan','Minnesota','Nebraska','North Dakota','Ohio',\n",
        "                           'South Dakota','Wisconsin'],\n",
        "               'Northeast': ['Connecticut','Delaware','Maine','Massachusetts','New Hampshire','New Jersey','New York',\n",
        "                            'Pennsylvania','Rhode Island','Vermont'],\n",
        "               'South': ['Alabama','Arkansas','District of Columbia','Florida','Georgia','Kentucky','Louisiana','Maryland',\n",
        "                        'Mississippi','Missouri','North Carolina','Oklahoma','South Carolina','Tennessee','Texas','Virginia',\n",
        "                        'West Virginia'],\n",
        "               'West': ['Alaska','Arizona','California','Colorado','Hawaii','Idaho','Montana','Nevada','New Mexico','Oregon',\n",
        "                       'Utah','Washington','Wyoming']}\n",
        "\n",
        "#flattening out dictionary to map each region to each individual state\n",
        "flat_regions = {val:key for key, lst in regions_dict.items() for val in lst}\n",
        "\n",
        "#adding regions to new income dataframe\n",
        "incomeDF['Region'] = incomeDF['State'].map(flat_regions)\n",
        "\n",
        "#calculating US average (since that data is not automatically included in the income dataset)\n",
        "US_average = incomeDF.iloc[:,1:-1].mean().mean()\n",
        "\n",
        "#grouping income data by region\n",
        "income_grouped = incomeDF.groupby('Region')\n",
        "income_median_averages = income_grouped.mean().reset_index()\n",
        "\n",
        "#getting median values for each region in the income dataset\n",
        "income_median_averages.index = income_median_averages['Region']\n",
        "income_median_averages = income_median_averages.iloc[:,1:].transpose()\n",
        "income_list = income_median_averages.mean().tolist()\n",
        "income_region_list = [income_list[0],income_list[1],US_average,income_list[2],income_list[3]]\n",
        "\n",
        "#adding income data to regions dataframe and taking away housing data\n",
        "regionsDF = regionsDF.iloc[:,:-2]\n",
        "regionsDF['IncomeMedian'] = income_region_list\n",
        "\n",
        "display(regionsDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCGrVt1pT6Xa"
      },
      "source": [
        "#Normalization\n",
        "norm = (regionsDF - regionsDF.mean())/regionsDF.std()\n",
        "\n",
        "#L1 Distance Matrix\n",
        "normHD = pairs.manhattan_distances(norm)\n",
        "norm = pd.DataFrame(normHD, columns=norm.index, index = norm.index)\n",
        "norm #high dimensional distances for every region\n",
        "\n",
        "#MDS calculations\n",
        "mds = mani.MDS(dissimilarity='precomputed', n_init=10, max_iter=1000)\n",
        "data2D = mds.fit_transform(norm)\n",
        "data2D = pd.DataFrame(data2D, columns=['x','y'], index=norm.index)\n",
        "data2D #suggested coordinates for plotting in two dimensions\n",
        "\n",
        "#Plotting coordinate suggestions\n",
        "ax = data2D.plot.scatter(x='x',y='y')\n",
        "for i in range(len(data2D)):\n",
        "    ax.text(data2D.x[i]+.2, data2D.y[i]-.1, data2D.index[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLl-9rx1T8d2"
      },
      "source": [
        "### MDS on Avocado, Housing, and Income Data\n",
        "\n",
        "#adding back in housing data\n",
        "regionsDF['HousingMeanPrice'] = region_means\n",
        "regionsDF['HousingMedianPrice'] = region_medians\n",
        "\n",
        "display(regionsDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP8tM_hLT-hW"
      },
      "source": [
        "#Normalization\n",
        "norm = (regionsDF - regionsDF.mean())/regionsDF.std()\n",
        "\n",
        "#L1 Distance Matrix\n",
        "normHD = pairs.manhattan_distances(norm)\n",
        "norm = pd.DataFrame(normHD, columns=norm.index, index = norm.index)\n",
        "norm #high dimensional distances for every region\n",
        "\n",
        "#MDS calculations\n",
        "mds = mani.MDS(dissimilarity='precomputed', n_init=10, max_iter=1000)\n",
        "data2D = mds.fit_transform(norm)\n",
        "data2D = pd.DataFrame(data2D, columns=['x','y'], index=norm.index)\n",
        "data2D #suggested coordinates for plotting in two dimensions\n",
        "\n",
        "#Plotting coordinate suggestions\n",
        "ax = data2D.plot.scatter(x='x',y='y')\n",
        "for i in range(len(data2D)):\n",
        "    ax.text(data2D.x[i]+.2, data2D.y[i]-.1, data2D.index[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRMKKgTWHJlC"
      },
      "source": [
        "### Sentiment Analysis of Avocado Tweets (Ryland Hanson)\n",
        "\n",
        "One interesting method of analyzing this twitter data would be to track the sentiment of the tweets and group them by locations over time. Ffirst we can create a bag of words from each tweet, clean them up, remove stopwords, then using a list of positive and negative words we can compute a value for the sentiment. In this case, positive sentement is classified as a sentiment value at least 0.1, a negative sentement would be less than -0.1, with the rest being considered neutral. We can then visualize this by breaking up into the cities and seeing the distribution of sentiment, we can group it by time and see how sentiment changes over time, and we can combine them in many possible ways to show various information about the sentiment in the various locations over the 5 year period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F2o-CyMHU6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "30a8fb15-f6b4-4ee6-d912-0300246dadf8"
      },
      "source": [
        "## Adds a length of bag feature to the DF\n",
        "cleaned_tweets['Length'] = cleaned_tweets['bag'].map(len)\n",
        "\n",
        "## Our list of positive and negative words to test the sentiment\n",
        "positives = set(open('positive.txt').read().split()) \n",
        "negatives = set(open('negative.txt').read().split())\n",
        "\n",
        "## Calculates the sentiment value for each tweet in the DF\n",
        "pos_bag = cleaned_tweets.bag.map(lambda bag: [word for word in bag if word in positives])\n",
        "neg_bag = cleaned_tweets.bag.map(lambda bag: [word for word in bag if word in negatives])\n",
        "cleaned_tweets['sentiment_value'] = (pos_bag.map(len) - neg_bag.map(len)) / cleaned_tweets.Length \n",
        "\n",
        "## Assign a sentiment grouping based on the value\n",
        "cleaned['sentiment'] = cleaned.sentiment_value.map(lambda x: 'negative' if x < -0.1 else 'positive' if x > 0.1 else 'neutral')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d047547ba640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Adds a length of bag feature to the DF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcleaned_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## Our list of positive and negative words to test the sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpositives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'positive.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cleaned_tweets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXEu31_HKfGj"
      },
      "source": [
        "## Seperate the tweets by year to visualize\n",
        "tweets_2015 = cleaned_tweets[cleaned_tweets.date.str.startswith('2015')]\n",
        "tweets_2016 = cleaned_tweets[cleaned_tweets.date.str.startswith('2016')]\n",
        "tweets_2017 = cleaned_tweets[cleaned_tweets.date.str.startswith('2017')]\n",
        "tweets_2018 = cleaned_tweets[cleaned_tweets.date.str.startswith('2018')]\n",
        "tweets_2019 = cleaned_tweets[cleaned_tweets.date.str.startswith('2019')]\n",
        "\n",
        "tweets_2015"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vep-E8rUKkfa"
      },
      "source": [
        "## groupby objects for each year to plot in bar graphs\n",
        "sentiment_2015 = tweets_2015.groupby(['city', 'sentiment']).sentiment.count().unstack()\n",
        "sentiment_2016 = tweets_2016.groupby(['city', 'sentiment']).sentiment.count().unstack()\n",
        "sentiment_2017 = tweets_2017.groupby(['city', 'sentiment']).sentiment.count().unstack()\n",
        "sentiment_2018 = tweets_2018.groupby(['city', 'sentiment']).sentiment.count().unstack()\n",
        "sentiment_2019 = tweets_2019.groupby(['city', 'sentiment']).sentiment.count().unstack()\n",
        "\n",
        "## Set up the plotting. Note 2019 will go on seperate row due to odd number of plots\n",
        "fig, axs = plt.subplots(2, 2, figsize = (10, 8), sharey=True)\n",
        "\n",
        "## Plot the bar charts\n",
        "sentiment_2015.plot(kind=\"bar\", ax = axs[0, 0], title = '2015')\n",
        "sentiment_2016.plot(kind=\"bar\", ax = axs[0, 1], title = '2016')\n",
        "sentiment_2017.plot(kind=\"bar\", ax = axs[1, 0], title = '2017')\n",
        "sentiment_2018.plot(kind=\"bar\", ax = axs[1, 1], title = '2018')\n",
        "\n",
        "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "    \n",
        "## Plot the final year\n",
        "sentiment_2019.plot(kind=\"bar\", title = '2019', figsize=(5, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvNG0WXgPXd8"
      },
      "source": [
        "## Another visualization, while a bit scattered on one plot, can be given to show the change of sentiment over time\n",
        "cleaned_tweets['day'] = cleaned_tweets.date.map(lambda date: date.split()[0]) ## Add a column for the day, to generarilize by day instead of exact time\n",
        "ts_df = cleaned_tweets.groupby(['day', 'city']).sentiment_value.mean().unstack() ## Group by to get a df with sentiment values by day and city\n",
        "ax = ts_df.iloc[1:, :].plot(figsize = (10, 8))\n",
        "plt.axhline(y=0.1, color='black', linestyle='--', label = 'Upper bound of neutral') ## Plot the threshold for positive sentiment\n",
        "plt.axhline(y=-.1, color='black', linestyle='--', label = 'Lower bound of neutral') ## Plot the threshold for negative sentiment\n",
        "plt.ylabel('Sentiment Value')\n",
        "plt.xlabel('Day')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QOXGrr6C9Aa"
      },
      "source": [
        "## Formalized Answers from Analysis\n",
        "\n",
        "### Write a paragraph about what important things you observed, what question(s) your analysis is answering, and how it relates to some of the other analyses performed by others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aPEE_iqE0F7"
      },
      "source": [
        "### Paolo Fermin\n",
        "\n",
        "#### One of the research questions we attempted to answer was whether avocados are an elastic good. This stems from the common perception in the media that avocados are inelastic, and that young people will buy them no matter the price. \n",
        "\n",
        "#### What we can learn from the scatterplots above is that the demand for organic avocados is elastic only in certain locations around the country. At locations where R2 values have magnitudes above 0.5, the data seems to lie close to the linear regression line. Once the magnitude of R2 falls below that threshold, the data is scattered further from the regression line. Organic avocados in these places are inelastic, and the price has little influence on the demand. \n",
        "\n",
        "#### Interestingly, this correlation is not seen in the aggregate \"Total U.S.\" data - in fact, the Total U.S. correlation is worse than any particular geography. This proves that simply looking at aggregate data does not tell the whole story of this dataset. Between locations in the U.S. are vast differences in income, cost of living, and many other factors that cannot be accounted for when looking at a very high level. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7chBm4wgE47G"
      },
      "source": [
        "### Somya Jain\n",
        "\n",
        "One of our research questions was \"how does avocado purchasing relate to average income in different regions?\" To answer this we first looked at avocado purchasing patterns between 2015 and 2019 and then analyzed the average median in the 5 regions and the prices of avocados in those same regions over different years. We found that that for all regions, the prices of avocados peaked around 2017. We also found that there is a weak positive correlation of about 0.53 between average median income and average avocado prices. This relates to the MDS analysis done on avocado and income data which found that Midwest and South were similar in their purchaing patterns. This is reflected in the scatter plot as well since the points for the Midwest and South are closer to each other. These observations are important because it shows that as incomes go up, so do the prices of avocados. This indicates that people (including millenials) can afford to buy avocados despite them being \"unnecessary,\" which further suggests that they are not taking away from spending on more \"important\" purchases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPeDI-N6uO9Y"
      },
      "source": [
        "fig_prices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMzlpC_SuPKP"
      },
      "source": [
        "fig_incomes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFScsgt8uPUZ"
      },
      "source": [
        "print('Correlation Coefficient:',corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e99lB6d-E8JE"
      },
      "source": [
        "### Shannon Aikens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvknRzHp2wAn"
      },
      "source": [
        "Building off of question 1, we also wanted to compare avocado purchasing to the housing market in different regions. Some of the assumptions made on the internet are that people who buy avocados are wasting money they could be putting towards a house, or people who are rich enough to buy avocados also are rich enough to purchase houses. To validate either of these assumptions, we needed to look at how the housing market and avocado spending habits impacted the similarity of regions. Using MDS, I found that the Midwest and South were most similar based on housing and avocado data. These regions were also closest to the US averages which gives an indication that these lower values are skewing the country averages towards lower prices and volumes. The Northeast was not only far away from the other regions but also far away from the US averages. Looking at the actual data, the Northeast has the highest avocado prices and median volume as well as the highest housing prices which supports the second claim that those who can afford avocados are those who can also afford more expensive houses. \n",
        "\n",
        "Since the finding that the South and Midwest are very similar regions is also a conclusion made from question 1, I wanted to add in income by region to the MDS model to see if the coordinates of the regional scatterplot shifted drastically. The South and Midwest got even closer with the income data added as factors which shows that there is a strong correlation between avocados, income, and housing as we had initially predicted. The Northeast still stood out among the rest due to its very high values in all categories. Since this general pattern was witnessed throughout all three MDS models, it shows that avocado data differs between the regions similarly to both income and housing data. The higher the income or housing prices, the higher the avocado prices and consumption will be. This shows that people will use their expendable income on avocados just as much as they will houses, so avocado purchasing isn't necessarily getting in the way of a boom in the housing market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjZfPNU0E-Uf"
      },
      "source": [
        "### Ryland Hanson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1dpbSlC5Ci4"
      },
      "source": [
        "One of the initial questions was how sentiment on avacados may be in some way related to the market, be it by sale volume or price. While difficult to accurately measure the nationwide sentiment on something specific like avocados, we explored this through the use of scraping tweets that mention the word avocado from different geo stamped locations and then measuring the sentiment with a bag of words approach. What we found in this case is that the sentiment of tweets in regards to avocados are largely fairly neutral, there are a few things we can notice. When looking at how the average price for the various cities change with time, we can see that some cities exhibit a familiar behavior along with the sentiment: when prices are high, there may be more negative (or at least less positive) sentiment, but when prices lower often times the negative sentiment will as well. An example in the plot below shows New York's average price starts to go down from 2017-2018, and in the sentiment bar chart we see that while the change is quite small, especially graphically, we can see that indeed the negative sentiment went down a bit and neutral seemed to increase. In the price plot we can see that in 2019 Houston had a big jump in price, and indeed they also had a big jump in negative sentiment, despite the positive also increasing a bit. This relationship can be seen in the rest of the cities too at certain jumps in price, therefore we can indeed say there does seem to be some form of relationship between the perceived sentiment of avocados at certain locations and times, and the average pricing. This result makes sense as well, people tend to be more happy (positive) about a product when it is cheap rather than unduly expensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GggMzyOk4NQt"
      },
      "source": [
        "## Read in the avocado pricing dataset\n",
        "price_df = pd.read_csv('cleaned_avocados.csv')\n",
        "\n",
        "## Retrieve just the organic pricing for consistency\n",
        "organic = price_df[(price_df.type == 'organic')]\n",
        "\n",
        "## Group by the year and location, then average the price for the year\n",
        "gb = organic.groupby(['year', 'geography']).average_price.mean().unstack()\n",
        "\n",
        "## Select only the cities with sentiment information\n",
        "cities_avg = gb[['New York', 'Atlanta', 'Chicago', 'Houston', 'Los Angeles', 'Miami/Ft. Lauderdale']]\n",
        "\n",
        "## Plot the average price over time for each location\n",
        "cities_avg.plot(figsize = (9, 6))\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Ax4iku48di"
      },
      "source": [
        "## Compare with bar graphs created in prior section:\n",
        "fig, axs = plt.subplots(2, 2, figsize = (10, 8), sharey=True)\n",
        "\n",
        "sentiment_2015.plot(kind=\"bar\", ax = axs[0, 0], title = '2015')\n",
        "sentiment_2016.plot(kind=\"bar\", ax = axs[0, 1], title = '2016')\n",
        "sentiment_2017.plot(kind=\"bar\", ax = axs[1, 0], title = '2017')\n",
        "sentiment_2018.plot(kind=\"bar\", ax = axs[1, 1], title = '2018')\n",
        "\n",
        "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "\n",
        "sentiment_2019.plot(kind=\"bar\", title = '2019', figsize=(5, 4))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30CV0tmUFBtb"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahneqmkfFexr"
      },
      "source": [
        "## Shannon will whip this up last minute :)"
      ]
    }
  ]
}